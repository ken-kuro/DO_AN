{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job - Pre-processing and Modelling Iteration final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "from datetime import date\n",
    "from time import strptime\n",
    "\n",
    "import RAKE as rake\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################\n",
    "\n",
    "# Working on Job description Data\n",
    "######################################################################################   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading my sorted job csv\n",
    "job = pd.read_csv('WIP/sorted_jobs_master_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38941 entries, 0 to 38940\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   company           38941 non-null  object \n",
      " 1   education         38941 non-null  object \n",
      " 2   experience        38941 non-null  int64  \n",
      " 3   industry          38941 non-null  object \n",
      " 4   jobdescription    38941 non-null  object \n",
      " 5   jobtitle          38941 non-null  object \n",
      " 6   payrate           38941 non-null  object \n",
      " 7   skills            38941 non-null  object \n",
      " 8   experience_range  38941 non-null  int64  \n",
      " 9   industry_enum     38941 non-null  int64  \n",
      " 10  Salary_range      38941 non-null  float64\n",
      " 11  j_id              38941 non-null  int64  \n",
      " 12  is_grad           38941 non-null  int64  \n",
      " 13  is_postgrad       38941 non-null  int64  \n",
      " 14  is_doc            38941 non-null  int64  \n",
      " 15  location          38941 non-null  int64  \n",
      " 16  loc_name          38941 non-null  object \n",
      "dtypes: float64(1), int64(8), object(8)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "job.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################################################################################\n",
    "# Understanding Job_description column (using NLP)\n",
    "###########################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. NLP - NLTK application to understand most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "set(stopwords.words('english'))\n",
    "# nltk.download('abc')\n",
    "# from nltk.corpus import abc\n",
    "# from nltk import RegexpTokenizer\n",
    "\n",
    "import string\n",
    "stopwords = set(stopwords.words(\"english\"))\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining tokenizer \n",
    "def my_tokenizer(text):\n",
    "    # 1. split at whitespace\n",
    "    text = text.split(' ')\n",
    "    \n",
    "    #2. lowercase\n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    #3. Remove puncutation\n",
    "    #table to replace puncuation\n",
    "    punc_table = str.maketrans('','',string.punctuation)\n",
    "    \n",
    "    #call translate()\n",
    "    text = [word.translate(punc_table) for word in text]\n",
    "    \n",
    "    #4. remove stopwords\n",
    "    text = [word for word in text if word not in stopwords]\n",
    "    \n",
    "    #5. lemmmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lemmatizer.lemmatize(word, pos='v') for word in text]\n",
    "    text = [lemmatizer.lemmatize(word, pos='n') for word in text]\n",
    "    text = [lemmatizer.lemmatize(word, pos='a') for word in text]\n",
    "    \n",
    "    #6. remove empty strings\n",
    "    text = [word for word in text if word !='']\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. NLP - TF-IDF application to get a list of all tokens \n",
    "-- This helped to gather what words needed to be in stop-words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = job['jobdescription'].str.rstrip('job description   send me jobs like this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Qualifications: - == > 10th To Graduation & A...\n",
       "1         Qualifications: - == > 10th To Graduation & A...\n",
       "2         - as a developer in providing application des...\n",
       "3         - Involved with all stages of indirect taxati...\n",
       "4         - Involved with all stages of indirect taxati...\n",
       "                               ...                        \n",
       "38936     Looking for candidates with strong programmin...\n",
       "38937     Work with tech lead to architect and develop ...\n",
       "38938     We are looking for a Senior UI Developers and...\n",
       "38939     We are looking for a Senior UI Developers and...\n",
       "38940     Job description : Experience of 5-10 years wi...\n",
       "Name: jobdescription, Length: 38941, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# job['jobdescription'] = job.jobdescription.str[40:]\n",
    "job['jobdescription']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t= job.copy()\n",
    "# t.to_csv('WIP.sorted_jobs_master_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shail\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>company</th>\n",
       "      <th>jd_combo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>walkin data entry operator (night shift)</td>\n",
       "      <td>MM Media Pvt Ltd</td>\n",
       "      <td>walkin data entry operator (night shift)  Qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>work based onhome based part time.</td>\n",
       "      <td>find live infotech</td>\n",
       "      <td>work based onhome based part time.  Qualificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pl/sql developer - sql</td>\n",
       "      <td>Softtech Career Infosystem Pvt. Ltd</td>\n",
       "      <td>pl/sql developer - sql  - as a developer in pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca  - Invo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca  - Invo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   j_id                                  jobtitle  \\\n",
       "0     0  walkin data entry operator (night shift)   \n",
       "1     1        work based onhome based part time.   \n",
       "2     2                    pl/sql developer - sql   \n",
       "3     3    manager/ad/partner - indirect tax - ca   \n",
       "4     4    manager/ad/partner - indirect tax - ca   \n",
       "\n",
       "                               company  \\\n",
       "0                     MM Media Pvt Ltd   \n",
       "1                   find live infotech   \n",
       "2  Softtech Career Infosystem Pvt. Ltd   \n",
       "3               Onboard HRServices LLP   \n",
       "4               Onboard HRServices LLP   \n",
       "\n",
       "                                            jd_combo  \n",
       "0  walkin data entry operator (night shift)  Qual...  \n",
       "1  work based onhome based part time.  Qualificat...  \n",
       "2  pl/sql developer - sql  - as a developer in pr...  \n",
       "3  manager/ad/partner - indirect tax - ca  - Invo...  \n",
       "4  manager/ad/partner - indirect tax - ca  - Invo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job_descriptions = job[['j_id','jobtitle','company' ]]\n",
    "df_job_descriptions['jd_combo'] = job['jobtitle'] +\" \" +  job['jobdescription'] \n",
    "df_job_descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shail\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ëœ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38941, 58510)\n",
      "(38941, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('ã¯æ’ëœ')\n",
    "#Transforms words to TFIDF\n",
    "vectorizer = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "index = 0\n",
    "keys = {}\n",
    "\n",
    "for jd in df_job_descriptions.itertuples() :\n",
    "    key = jd[1]\n",
    "    keys[key] = index\n",
    "    index += 1\n",
    "\n",
    "#Fit the vectorizer to the data\n",
    "vectorizer.fit(df_job_descriptions['jd_combo'].fillna(''))\n",
    "\n",
    "#Transform the data\n",
    "tfidf_scores = vectorizer.transform(df_job_descriptions['jd_combo'].fillna(''))\n",
    "\n",
    "print(tfidf_scores.shape)\n",
    "print(df_job_descriptions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tfidf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(tfidf_scores.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000</th>\n",
       "      <th>0000gmt</th>\n",
       "      <th>0001pt</th>\n",
       "      <th>00029</th>\n",
       "      <th>00034</th>\n",
       "      <th>000402</th>\n",
       "      <th>00053</th>\n",
       "      <th>...</th>\n",
       "      <th>ïƒ</th>\n",
       "      <th>ïƒ¼</th>\n",
       "      <th>ïƒž</th>\n",
       "      <th>œ100</th>\n",
       "      <th>œmost</th>\n",
       "      <th>œrecognition</th>\n",
       "      <th>œto</th>\n",
       "      <th>šâ</th>\n",
       "      <th>šã</th>\n",
       "      <th>žâ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58510 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000  0000  00000  0000gmt  0001pt  00029  00034  000402  00053  \\\n",
       "0  0.0  0.056499   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "1  0.0  0.068273   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "2  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "3  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "4  0.0  0.000000   0.0    0.0      0.0     0.0    0.0    0.0     0.0    0.0   \n",
       "\n",
       "   ...   ïƒ  ïƒ¼  ïƒž  œ100  œmost  œrecognition  œto   šâ   šã   žâ  \n",
       "0  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "1  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "2  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "3  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "4  ...  0.0  0.0  0.0   0.0    0.0           0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 58510 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As count vectorizer and Tf-Idf are only exploding my column numbers. It might not be wise to proceed with any of these. Moveover, I need to compare job description with Resume, that may not with fair comparison. So I will use these results so far for customizing stop word list. And will later use Doc2Vec to train my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating my Stopword list \n",
    "\n",
    "### As seen there are so many unwanted tokens like numbers,ïƒ¼ etc , I need to add them in \"stop words\" list to train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting list of all tokens\n",
    "word_list = test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting a list of unwanted words as s_words and adding to stopwords\n",
    "s_words =[]\n",
    "for word in word_list:\n",
    "    #print(word)\n",
    "    if re.search(\"^\\W|^\\d\",word):\n",
    "        s_words.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_words.append('')        \n",
    "from nltk.corpus import stopwords\n",
    "stopword_set = set(stopwords.words('english'))\n",
    "stopword_set = list(stopword_set)\n",
    "stopword_set.extend(s_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting all text data for DOC2VEC modelling\n",
    "In final iteration, I only used job title and job description for creating text combo document and got my 20-D vectors. This time I trained my model on 200 epochs. \n",
    "\n",
    "As count vectorizer and Tf-Idf are only exploding my column numbers. It might not be wise to proceed with any of these. Moveover, I need to compare job description with Resume, that may not with fair comparison. \n",
    "\n",
    "Definately Doc2Vec is the smart choice to make to proceed with matching. Because Doc2Vec has ability to read document as a whole rather than working on each single word. It has a feature to provide n-Dimentional vectors. \n",
    "\n",
    "So I am going to use same concept to get my vectors. Then I ll use those vectors to match it against any given resume. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j_id</th>\n",
       "      <th>jobtitle</th>\n",
       "      <th>company</th>\n",
       "      <th>jd_combo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>walkin data entry operator (night shift)</td>\n",
       "      <td>MM Media Pvt Ltd</td>\n",
       "      <td>walkin data entry operator (night shift)  Qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>work based onhome based part time.</td>\n",
       "      <td>find live infotech</td>\n",
       "      <td>work based onhome based part time.  Qualificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pl/sql developer - sql</td>\n",
       "      <td>Softtech Career Infosystem Pvt. Ltd</td>\n",
       "      <td>pl/sql developer - sql  - as a developer in pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca  - Invo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca</td>\n",
       "      <td>Onboard HRServices LLP</td>\n",
       "      <td>manager/ad/partner - indirect tax - ca  - Invo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   j_id                                  jobtitle  \\\n",
       "0     0  walkin data entry operator (night shift)   \n",
       "1     1        work based onhome based part time.   \n",
       "2     2                    pl/sql developer - sql   \n",
       "3     3    manager/ad/partner - indirect tax - ca   \n",
       "4     4    manager/ad/partner - indirect tax - ca   \n",
       "\n",
       "                               company  \\\n",
       "0                     MM Media Pvt Ltd   \n",
       "1                   find live infotech   \n",
       "2  Softtech Career Infosystem Pvt. Ltd   \n",
       "3               Onboard HRServices LLP   \n",
       "4               Onboard HRServices LLP   \n",
       "\n",
       "                                            jd_combo  \n",
       "0  walkin data entry operator (night shift)  Qual...  \n",
       "1  work based onhome based part time.  Qualificat...  \n",
       "2  pl/sql developer - sql  - as a developer in pr...  \n",
       "3  manager/ad/partner - indirect tax - ca  - Invo...  \n",
       "4  manager/ad/partner - indirect tax - ca  - Invo...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_job_descriptions = job[['j_id','jobtitle','company' ]]\n",
    "# df_job_descriptions['jd_combo'] = job['jobtitle'] +\" \" +  job['jobdescription'] \n",
    "df_job_descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    walkin data entry operator (night shift)  Qual...\n",
       "1    work based onhome based part time.  Qualificat...\n",
       "2    pl/sql developer - sql  - as a developer in pr...\n",
       "3    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "4    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "5    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "6    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "7    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "8    manager/ad/partner - indirect tax - ca  - Invo...\n",
       "9    java technical lead (6-8 yrs) -  Please share ...\n",
       "Name: jd_combo, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df_job_descriptions['jd_combo']\n",
    "docs_sample = docs.head(10)\n",
    "docs_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing with custom stop word list\n",
    "def preprocess(text):\n",
    "    stop_words = stopword_set\n",
    "    #0. split words by whitespace\n",
    "    text = text.split()\n",
    "    \n",
    "    \n",
    "    # 1. lower case\n",
    "    text = [word.lower() for word in text]\n",
    "    \n",
    "    # 2. remove punctuations\n",
    "    punc_table = str.maketrans('','',string.punctuation)\n",
    "    text = [word.translate(punc_table) for word in text]\n",
    "    \n",
    "    # 3. remove stop words\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling my pre-process to tokenize \n",
    "tokenized_doc = []\n",
    "doc = df_job_descriptions['jd_combo']\n",
    "#doc = docs_sample\n",
    "for d in doc:\n",
    "    tokenized_doc.append(preprocess(d))\n",
    "#tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized document into gensim formated tagged data\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38941"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_doc = len(tagged_data)\n",
    "num_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#settings to show epoch progress\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self, path_prefix):\n",
    "        self.path_prefix = path_prefix\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        output_path = get_tmpfile('{}_epoch{}.model'.format(self.path_prefix, self.epoch))\n",
    "        model.save(output_path)\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings to show epoch progress\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n",
      "Epoch #50 start\n",
      "Epoch #50 end\n",
      "Epoch #51 start\n",
      "Epoch #51 end\n",
      "Epoch #52 start\n",
      "Epoch #52 end\n",
      "Epoch #53 start\n",
      "Epoch #53 end\n",
      "Epoch #54 start\n",
      "Epoch #54 end\n",
      "Epoch #55 start\n",
      "Epoch #55 end\n",
      "Epoch #56 start\n",
      "Epoch #56 end\n",
      "Epoch #57 start\n",
      "Epoch #57 end\n",
      "Epoch #58 start\n",
      "Epoch #58 end\n",
      "Epoch #59 start\n",
      "Epoch #59 end\n",
      "Epoch #60 start\n",
      "Epoch #60 end\n",
      "Epoch #61 start\n",
      "Epoch #61 end\n",
      "Epoch #62 start\n",
      "Epoch #62 end\n",
      "Epoch #63 start\n",
      "Epoch #63 end\n",
      "Epoch #64 start\n",
      "Epoch #64 end\n",
      "Epoch #65 start\n",
      "Epoch #65 end\n",
      "Epoch #66 start\n",
      "Epoch #66 end\n",
      "Epoch #67 start\n",
      "Epoch #67 end\n",
      "Epoch #68 start\n",
      "Epoch #68 end\n",
      "Epoch #69 start\n",
      "Epoch #69 end\n",
      "Epoch #70 start\n",
      "Epoch #70 end\n",
      "Epoch #71 start\n",
      "Epoch #71 end\n",
      "Epoch #72 start\n",
      "Epoch #72 end\n",
      "Epoch #73 start\n",
      "Epoch #73 end\n",
      "Epoch #74 start\n",
      "Epoch #74 end\n",
      "Epoch #75 start\n",
      "Epoch #75 end\n",
      "Epoch #76 start\n",
      "Epoch #76 end\n",
      "Epoch #77 start\n",
      "Epoch #77 end\n",
      "Epoch #78 start\n",
      "Epoch #78 end\n",
      "Epoch #79 start\n",
      "Epoch #79 end\n",
      "Epoch #80 start\n",
      "Epoch #80 end\n",
      "Epoch #81 start\n",
      "Epoch #81 end\n",
      "Epoch #82 start\n",
      "Epoch #82 end\n",
      "Epoch #83 start\n",
      "Epoch #83 end\n",
      "Epoch #84 start\n",
      "Epoch #84 end\n",
      "Epoch #85 start\n",
      "Epoch #85 end\n",
      "Epoch #86 start\n",
      "Epoch #86 end\n",
      "Epoch #87 start\n",
      "Epoch #87 end\n",
      "Epoch #88 start\n",
      "Epoch #88 end\n",
      "Epoch #89 start\n",
      "Epoch #89 end\n",
      "Epoch #90 start\n",
      "Epoch #90 end\n",
      "Epoch #91 start\n",
      "Epoch #91 end\n",
      "Epoch #92 start\n",
      "Epoch #92 end\n",
      "Epoch #93 start\n",
      "Epoch #93 end\n",
      "Epoch #94 start\n",
      "Epoch #94 end\n",
      "Epoch #95 start\n",
      "Epoch #95 end\n",
      "Epoch #96 start\n",
      "Epoch #96 end\n",
      "Epoch #97 start\n",
      "Epoch #97 end\n",
      "Epoch #98 start\n",
      "Epoch #98 end\n",
      "Epoch #99 start\n",
      "Epoch #99 end\n",
      "Epoch #100 start\n",
      "Epoch #100 end\n",
      "Epoch #101 start\n",
      "Epoch #101 end\n",
      "Epoch #102 start\n",
      "Epoch #102 end\n",
      "Epoch #103 start\n",
      "Epoch #103 end\n",
      "Epoch #104 start\n",
      "Epoch #104 end\n",
      "Epoch #105 start\n",
      "Epoch #105 end\n",
      "Epoch #106 start\n",
      "Epoch #106 end\n",
      "Epoch #107 start\n",
      "Epoch #107 end\n",
      "Epoch #108 start\n",
      "Epoch #108 end\n",
      "Epoch #109 start\n",
      "Epoch #109 end\n",
      "Epoch #110 start\n",
      "Epoch #110 end\n",
      "Epoch #111 start\n",
      "Epoch #111 end\n",
      "Epoch #112 start\n",
      "Epoch #112 end\n",
      "Epoch #113 start\n",
      "Epoch #113 end\n",
      "Epoch #114 start\n",
      "Epoch #114 end\n",
      "Epoch #115 start\n",
      "Epoch #115 end\n",
      "Epoch #116 start\n",
      "Epoch #116 end\n",
      "Epoch #117 start\n",
      "Epoch #117 end\n",
      "Epoch #118 start\n",
      "Epoch #118 end\n",
      "Epoch #119 start\n",
      "Epoch #119 end\n",
      "Epoch #120 start\n",
      "Epoch #120 end\n",
      "Epoch #121 start\n",
      "Epoch #121 end\n",
      "Epoch #122 start\n",
      "Epoch #122 end\n",
      "Epoch #123 start\n",
      "Epoch #123 end\n",
      "Epoch #124 start\n",
      "Epoch #124 end\n",
      "Epoch #125 start\n",
      "Epoch #125 end\n",
      "Epoch #126 start\n",
      "Epoch #126 end\n",
      "Epoch #127 start\n",
      "Epoch #127 end\n",
      "Epoch #128 start\n",
      "Epoch #128 end\n",
      "Epoch #129 start\n",
      "Epoch #129 end\n",
      "Epoch #130 start\n",
      "Epoch #130 end\n",
      "Epoch #131 start\n",
      "Epoch #131 end\n",
      "Epoch #132 start\n",
      "Epoch #132 end\n",
      "Epoch #133 start\n",
      "Epoch #133 end\n",
      "Epoch #134 start\n",
      "Epoch #134 end\n",
      "Epoch #135 start\n",
      "Epoch #135 end\n",
      "Epoch #136 start\n",
      "Epoch #136 end\n",
      "Epoch #137 start\n",
      "Epoch #137 end\n",
      "Epoch #138 start\n",
      "Epoch #138 end\n",
      "Epoch #139 start\n",
      "Epoch #139 end\n",
      "Epoch #140 start\n",
      "Epoch #140 end\n",
      "Epoch #141 start\n",
      "Epoch #141 end\n",
      "Epoch #142 start\n",
      "Epoch #142 end\n",
      "Epoch #143 start\n",
      "Epoch #143 end\n",
      "Epoch #144 start\n",
      "Epoch #144 end\n",
      "Epoch #145 start\n",
      "Epoch #145 end\n",
      "Epoch #146 start\n",
      "Epoch #146 end\n",
      "Epoch #147 start\n",
      "Epoch #147 end\n",
      "Epoch #148 start\n",
      "Epoch #148 end\n",
      "Epoch #149 start\n",
      "Epoch #149 end\n",
      "Epoch #150 start\n",
      "Epoch #150 end\n",
      "Epoch #151 start\n",
      "Epoch #151 end\n",
      "Epoch #152 start\n",
      "Epoch #152 end\n",
      "Epoch #153 start\n",
      "Epoch #153 end\n",
      "Epoch #154 start\n",
      "Epoch #154 end\n",
      "Epoch #155 start\n",
      "Epoch #155 end\n",
      "Epoch #156 start\n",
      "Epoch #156 end\n",
      "Epoch #157 start\n",
      "Epoch #157 end\n",
      "Epoch #158 start\n",
      "Epoch #158 end\n",
      "Epoch #159 start\n",
      "Epoch #159 end\n",
      "Epoch #160 start\n",
      "Epoch #160 end\n",
      "Epoch #161 start\n",
      "Epoch #161 end\n",
      "Epoch #162 start\n",
      "Epoch #162 end\n",
      "Epoch #163 start\n",
      "Epoch #163 end\n",
      "Epoch #164 start\n",
      "Epoch #164 end\n",
      "Epoch #165 start\n",
      "Epoch #165 end\n",
      "Epoch #166 start\n",
      "Epoch #166 end\n",
      "Epoch #167 start\n",
      "Epoch #167 end\n",
      "Epoch #168 start\n",
      "Epoch #168 end\n",
      "Epoch #169 start\n",
      "Epoch #169 end\n",
      "Epoch #170 start\n",
      "Epoch #170 end\n",
      "Epoch #171 start\n",
      "Epoch #171 end\n",
      "Epoch #172 start\n",
      "Epoch #172 end\n",
      "Epoch #173 start\n",
      "Epoch #173 end\n",
      "Epoch #174 start\n",
      "Epoch #174 end\n",
      "Epoch #175 start\n",
      "Epoch #175 end\n",
      "Epoch #176 start\n",
      "Epoch #176 end\n",
      "Epoch #177 start\n",
      "Epoch #177 end\n",
      "Epoch #178 start\n",
      "Epoch #178 end\n",
      "Epoch #179 start\n",
      "Epoch #179 end\n",
      "Epoch #180 start\n",
      "Epoch #180 end\n",
      "Epoch #181 start\n",
      "Epoch #181 end\n",
      "Epoch #182 start\n",
      "Epoch #182 end\n",
      "Epoch #183 start\n",
      "Epoch #183 end\n",
      "Epoch #184 start\n",
      "Epoch #184 end\n",
      "Epoch #185 start\n",
      "Epoch #185 end\n",
      "Epoch #186 start\n",
      "Epoch #186 end\n",
      "Epoch #187 start\n",
      "Epoch #187 end\n",
      "Epoch #188 start\n",
      "Epoch #188 end\n",
      "Epoch #189 start\n",
      "Epoch #189 end\n",
      "Epoch #190 start\n",
      "Epoch #190 end\n",
      "Epoch #191 start\n",
      "Epoch #191 end\n",
      "Epoch #192 start\n",
      "Epoch #192 end\n",
      "Epoch #193 start\n",
      "Epoch #193 end\n",
      "Epoch #194 start\n",
      "Epoch #194 end\n",
      "Epoch #195 start\n",
      "Epoch #195 end\n",
      "Epoch #196 start\n",
      "Epoch #196 end\n",
      "Epoch #197 start\n",
      "Epoch #197 end\n",
      "Epoch #198 start\n",
      "Epoch #198 end\n",
      "Epoch #199 start\n",
      "Epoch #199 end\n"
     ]
    }
   ],
   "source": [
    "#train model - final******** with 200 epochs\n",
    "epoch_logger = EpochLogger()\n",
    "## Train doc2vec model\n",
    "model1 = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 200, callbacks=[epoch_logger])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained doc2vec model\n",
    "model1.save(\"Model/my_doc2vec_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load saved doc2vec model\n",
    "model1= Doc2Vec.load(\"Model/my_doc2vec_v2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38941"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confirm length (should be 38941)\n",
    "len(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get vector value\n",
    "vec = np.empty([38941,20])\n",
    "\n",
    "for k,i in enumerate(tokenized_doc):\n",
    "    \n",
    "    #print(i)\n",
    "    vector = model1.infer_vector(i)\n",
    "    vec[k] = vector\n",
    "    #vec = np.append(vector)\n",
    "    #vecf = np.append(vec,vector)\n",
    "\n",
    "# reshape into 2D\n",
    "new_arr = np.reshape(vec,(-1,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = range(1, 21)\n",
    "vec_df = pd.DataFrame(new_arr, columns=['vec_' + str(i) for i in rng])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38941 entries, 0 to 38940\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   vec_1   38941 non-null  float64\n",
      " 1   vec_2   38941 non-null  float64\n",
      " 2   vec_3   38941 non-null  float64\n",
      " 3   vec_4   38941 non-null  float64\n",
      " 4   vec_5   38941 non-null  float64\n",
      " 5   vec_6   38941 non-null  float64\n",
      " 6   vec_7   38941 non-null  float64\n",
      " 7   vec_8   38941 non-null  float64\n",
      " 8   vec_9   38941 non-null  float64\n",
      " 9   vec_10  38941 non-null  float64\n",
      " 10  vec_11  38941 non-null  float64\n",
      " 11  vec_12  38941 non-null  float64\n",
      " 12  vec_13  38941 non-null  float64\n",
      " 13  vec_14  38941 non-null  float64\n",
      " 14  vec_15  38941 non-null  float64\n",
      " 15  vec_16  38941 non-null  float64\n",
      " 16  vec_17  38941 non-null  float64\n",
      " 17  vec_18  38941 non-null  float64\n",
      " 18  vec_19  38941 non-null  float64\n",
      " 19  vec_20  38941 non-null  float64\n",
      "dtypes: float64(20)\n",
      "memory usage: 5.9 MB\n"
     ]
    }
   ],
   "source": [
    "vec_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_job_1 = pd.concat([job, vec_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving final csv with additional vectors to match with resume. \n",
    "con_job_1.to_csv('wip/con_job_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
